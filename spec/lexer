     The lexer is responsible for taking a raw array of bytes and decomposing
them into the tokens we love and desire. Since brainfuck's grammar is 
considerably limited and anything not composing the grammar is a comment,
this should be easy. 
     Any value that doesn't compose our grammar will map to some invalid 
or NONE token.

     [ will map to OBRACKETS
     ] will map to CBRACKETS
     > will map to INCMEMP
     < will map to DECMEMP
     + will map to INCVAL
     - will map to DECVAL
     , will map to GETVAL
     . will map to PUTVAL 


     Implementation
     - We will have an enum for all the possible mutually exclusive tokens.
     - Defining a trait containing the basic operations provided by the 
     tokenizer seems to be the most reasonable choice.
     - We could implement the trait for the Stdio and File data structures,
     taking ownership of them
